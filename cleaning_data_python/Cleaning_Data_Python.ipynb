{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Cleaning Data with Python_\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnóstico\n",
    "\n",
    "Antes de realizar cualquier trabajo sobre nuestros datos es importante tomarnos un tiempo para explorar su naturaleza. Hay ciertos puntos que, por lo general, debemos tomar en cuenta sobre nuestros datos:\n",
    "\n",
    "* Nuestro dataset puede tener inconsistencias en el nombre de sus atributos (columnas), es decir; _–Probablemente los títulos tengan simbolos, espacios en blanco, caracteres erroneos, etc, etc...–_ todo aquello que impida una correcta identificación de ellas.\n",
    "\n",
    "* \"Missin Data\" o datos omitidos, por alguna razón, deben ser identificados y localizados.\n",
    "\n",
    "* \"Outliers\" suelen ser un proble potencial a la hora de trabajar con nuestros datos. Para ello es necesario conocer más sobre la naturaleza de nuestro dataset para después clasificarlos y entender _–¿ Cuales pueden ser un verdadero problema o cuales no ?.–_\n",
    "\n",
    "* Registros duplicados.\n",
    "\n",
    "* La asignación de formatos erroneos en los registros de los atributos (columnas) pueden traernos valores equivocados a la hora de procesarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('pagpoly.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseamos imprimir cierta parte de nuestro dataset, por que así lo hemos deseado, podemos echar mano de las siguientes métodos:\n",
    "\n",
    "* `DataFrame.head()` nos arrojará los primeros 5 renglones de nuestro dataset.\n",
    "\n",
    "* `DatarFrame.tail()` nos arrijará los últimos 5 renglones de nuestro dataset.\n",
    "\n",
    "* `DataFrame.shape` nos arroja el número de renglones y de columnas de nuestro dataset. Por ejemplo: –_El dataset **df** tiene un \"shape\" **(14478, 7)**. Es decir; que se compone de 14,478 regnglones y 7 atributos (columnas)–._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos de los atributos más importantes en _Pandas_, para un _DataFrame_, son `.columns` e `.info()`. Nos ayudan a identificar errores en los títulos de los atributos, caracteres erroneos o identificar valores omitidos por alguna razón o el tipo de formato en los regitros de los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los siguientes ejemplos es necesario aislar los siguiente atributos; `UNITNO`, `NAME` y `AGE` en un nuevo dataframe, al cual llamaremos `df_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=df[['UNITNO','STATE','NAME','AGE']]\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si volvemos a consultar la información de nuestro nuevo dataframe `df_subset` vemos que la columna `UNITNO` tiene un tipo de dato `object`. Esto es por que; _–el signo `*` antepuesto a las cantidades es conciderado como texto, lo cual impide representar a estos atributos como números–._ El siguiente paso será eliminarlos y cambiar el formato a dichos atributos por un caracter numérico.\n",
    "\n",
    "Si observamos el primer dataframe `df` nos encontramos que; la cantidad de objetos para cada atributo es de un total de **14,478**. Pero en los atributos restantes sus registros reportan una cantidad menor. Los registro de de estos atributos se concideran omitidos o sin ningúna representación por el momento. Por ejemplo: `MAP_SYMBOL`, `LITH1`, `LITH2`,`LITH3`... son algunos atributos con registros omitidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis explotratorio de datos.\n",
    "\n",
    "Hay dos maneras de realizar rapidamente un análisis exploratorio de nuestros datos. Una es contando las frecuencias de los valores únicos para cada registro. La otra es obteniendo estadísticas básicas en los atributos. _Python_ nos permite hacer esto mediante los métodos: `.value_counts(dropna=False)` y `df.describe()`\n",
    "\n",
    "Los siguientes métodos son aplicados para una exploración rápida en el dataset:\n",
    "\n",
    "1.- `DataFrame.column_name.value_counts(dropna=False)` nos arroja el conteo de las frecuencias para cada record. _–El parámetro `dropna = False` nos permite visualizar los valores que podrían ser omitidos en la columna.–_ \n",
    "\n",
    "2.- `DataFrame['columname'].value_counts(dropna=False)` es otra manera de realizar lo anterior.\n",
    "    \n",
    "3.- `DataFrame.describe()` arroja una estadística básica para cada uno de los atributos en el dataframe. Este método es solo se aplica cuando las columnas son de tipo numérico.\n",
    "\n",
    "Al aplicar estos métodos a los atributos `STATE` y `NAME` se observan los siguientes casos:\n",
    "\n",
    "* no todos los registros en el atributo `STATE` son de Pensilvania. Esto es curioso, sobre todo si decimos que el dataset contiene solamente aplicaciones para este estado.\n",
    "\n",
    "* en el atributo `AGE` observamos 4 valores `NaN` que habrá que investigar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.STATE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(print(df['AGE'].value_counts(dropna=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis explotratorio visual de datos.\n",
    "\n",
    "Hasta ahora hemos estado explorando analíticamente los registros para tratar de encontrar algúna anomalía _(outliers)_ en la información del dataset. Sin embargo podemos apoyarnos de graficos para poder explorar más estos casos.\n",
    "\n",
    "Los gráficos como _histogramas_ y _los boxplots_ son herramientas que nos ayudan en atributos que contienen datos númericos. Por ejemplo para el atributo `Existing Zoning Sqft` utlizamos la función **plot()**, de la librería _matplotlib_, con el parámetro `kind = hist` para poder imprimir un histograma. \n",
    "\n",
    "Antes comensaremos obteniendo rápidamente algunos datos estadísticos para su pre-evaluación y para ello harémos uso de un nuevo _Dataset_. Este nuevo archivo (`ssnmx_01-12_2019.csv`) contiene los registros sísmicos para la republica mexicana, en el periodo _enero 2019 - diciembre 2020_. Además se ha tratado la información para poder ejemplificar algunas cuestiones referentes a este tema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn=pd.read_csv('ssnmx_01-12_2019.csv')\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inmediatamente podemos hacer un barrido exploratorio con la función `info()` como ya lo vimos en ejemplos anteriores. Podrémos observar que nuestro _Dataset_ contiene `26425` registros (eventos). Sin embargo a simple vista no podríamos identificar datos que nos puedan ensuciar la información. Estos casos pueden ser; _outliers_, _incompatibilidad con el formato de sus atributos_, _valores_ `NaN` o `Null`, _entre otros_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin tener mucho conocimiento o experiencia en el _análisis exploratorio de datos_ podemos observar de primera cuenta que el atributo `Profundidad` está clasificado como `object` (_cadena_), a pesar de que los valores reportados en esta columna deberían ser números. _Python_ por defecto clasifica como `object` a cualquier registro que desde su archivo fuente (para este caso `ssnmx_01-12_2019.csv`) contenga caractéres o cadenas (_texto_) en sus atributos.\n",
    "\n",
    "Para tratar de solucionar esta observación podemos indicarle a _Pandas_ que abra nuestro archivo cambiando el formato del atributo `Profundidad` por `float` (decimal). El parámetro que debemos pasarle a pandas es `dtype={'atributo':type}`. Con esto deberíamos poder visualizar nuestro _DataFrame_ correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn=pd.read_csv('ssnmx_01-12_2019.csv', dtype={'Profundidad':float})\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no tener exito podemos pensar que no es cuestión del formato en nuestro atributo. Seguramente de debe a algo más. Muchas veces hay registros `NaN`,`Null`, registros con caractéres como `*`,`$`,`#` antepuestos, y como ya vimos en el ejemplo anterior no son posibles transformarlos.\n",
    "\n",
    "Si somos observadores en la imprsión de nuestro mensaje de error _Pandas_ nos arroja este mensaje:\n",
    "```pyhton\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read()\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_column_data()\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._convert_tokens()\n",
    "\n",
    "ValueError: could not convert string to float: 'menos de 1'\n",
    "```\n",
    "Lo que intenta decirnos este mensaje es que: _–existen registros que no pueden ser convertidos en números, tal es el caso de registros como `menos de 1`–_. Podemos hacer caso y buscar cuales son los registros que cumplen esta condición en. Para ello hay que hacer un filtrado en el atributo `Profundidad`, donde sus registros cumplan con la condición de ser cadenas de texto con el valor igual a `menos de 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=df_ssn[df_ssn['Profundidad']=='menos de 1']\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que efectivamente hay una cantidad pequeña de registros que cumplen esta condición. Aquí es donde nuestro sentido sobre el análisis que deseamos efectuar decidirá si eliminar estos datos o tratar de buscar una solución para no perderlos. \n",
    "\n",
    "Pero antes, nuestro objetivo era transformar el tipo de dato de estos valores de cadena a numérico independientemente si cumplian con la condición o no. Si volvemos a realizar un filtro al _DataFrame_\n",
    "`df_ssn` podemos seleccionar aquellos valores que cumplan con la condición opuesta, o sea, que se seleccionen aquellos registros, de en el atributo `Profundidad`, que no cumplan con la condición de ser iguales a `menos de 1`. Este nuevo _DataFrame_ lo llamarémos `df_ssn_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn_clean=df_ssn.loc[df_ssn['Profundidad']!='menos de 1']\n",
    "df_ssn_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseamos, hasta aquí, podemos trabajar con el resto de los datos sin esta condición. La fucnión `astype()` nos otorga la posibilidad de agregar un diccionario, como parámetro, para indicar que atributo y a que nuevo formato deseamos transformarlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn_clean = df_ssn_clean.astype({\"Profundidad\": float})\n",
    "df_ssn_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver el atributo profundidad cambio de ser `object` a `float`, como tenía que ser desde un principio. Ahora podemos inspeccionar un poco más nuestro _DataFrame_. Podemos utilizar la función `describe()` y obtener más información sobre el atributo `Profundidad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn_clean.Profundidad.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De primera cuenta podemos observar que hay una gran distancia entre el valor máximo y el valor mínimo en nuestros registros **min  1.0 km** y **max  60,000 km**. En primera; sería muy dificil tener confianza en un dato que provenga desde esa profundidad, sobre todo por que no existiría si tomamos en cuenta que el radio de la tierra es de **~6,300 kms** (aprox). EN segunda; estamos hablando de una profundidad mayor a la del núcleo interno, donde ya no existen placas tectónicas, todo se fusionó y se reciclo kilómetros arriba. Entonces es recomendable visualizar como se distribuyen las frecuencias de la `Prifundidad` de nuestros registros en un histograma. \n",
    "\n",
    "Para esta ocación ajustaremos logaritmicamente la escala del eje `y` para una visualización más comoda. Utilizamos entonces los parámetros `logy=True` en la función `plot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_ssn_clean['Profundidad'].plot(kind='hist', logy=True )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver; el **histograma**  nos ayuda a visualizar mejor la distribución de frecuencias de nuestros datos, un poco más que la forma analítica. Pero ninguna técnica es mejor que otra ya que antes de utilizar métodos gráficos necesitamos la descripción de nuestro _Dataset_ para esperar entender el resutado de este. Podemos hacernos la siguiente pregunta _–¿hacia que lado se encuentran más cargados nuestros registros?.–_\n",
    "\n",
    "En el gráfico podemos identificar (según nuestro rapido análisis analítico) que; el **25%, 50% y 75%** de los registros se encuentran hacia la izquierda, con valores menores a los `10,000 km` de `Profundidad`. Y sin embargo, a los lados del registro con mayor valor (última barra azul) no hay casi datos. Por lo cual diremos que este valor es considerado como anómalo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los _histográmas_ nos ayudan para visualizar solo una varaiable. Para visualizar multiples variables los _boxplots_ son una gran ayuda, especialmente cuando las varaiables son categóricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('dob_job_application_subset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo usaremos _boxplots_ para comparar `initial cost` a través de diferentes valores de `Borough` con ayuda del método `.boxplot()` de la librería de **pandas**: `df.boxplot(column='name_column', by='parameter', rot=90)`. Esta gráfica será el resultado de para una nueva variable llamada **df_boxplot** que alojará a los atributos `Initial Cost` y `Borough`.\n",
    "\n",
    "Ates de graficar es necesario realizar unos cambios en el formato del atributo `Initial Cost`:\n",
    "\n",
    "* Eliminar `$` del la columna `Initial Cost` : `ini_cost = df['Initial Cost']str.replace('$', '')`.\n",
    "\n",
    "* Convertir `Initial Cost` a formato `float`: `astype(float)`\n",
    "\n",
    "* Creación de la nueva variable **df_boxplot**: `df_boxplt= pd.concat([df['Borough'],ini_cost], axis=1)`.\n",
    "\n",
    "Una vez realizados estos cambios podemos construir la gráfica de la siguiente manera: `df_boxplt.boxplot(column='Initial Cost', by='Borough', rot=90)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_cost = df['Initial Cost'].str.replace('$', '').astype(float)\n",
    "df_boxplt= pd.concat([df['Borough'],ini_cost], axis=1)\n",
    "df_boxplt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxplt.boxplot(column='Initial Cost', by='Borough', rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que hay dos _outliers_ en _\"Manhatan\"_ esto se debe a que es uno de los lugares más caros y por lo tanto es normal. Este es un claro ejemplo cuando hay que tener en cuenta e identificar muy bien &mdash; _¿ qué consideramos ouliers y qué no ?_ &mdash;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Boxplot_ es una excelente opción cuando se tiene una columna, con valor numérico, y deseamos compararla con varias atributos. Pero cuando necesitamos visualizar 2 columnas numéricas _Scatter PLots_ es la mejor opción.  \n",
    "\n",
    "Imprimamos la gráfica con `Initial_cost`, en el eje de las `x` y `total_est_fee` en el eje de las `y`. Utilizaremos el método `.plot` con el parámetro `Kind='scatter'` y noteremos que hay dos _outliers_ que se alcanzan a visualizar. Para ello necesitamos realizar un proceso parecido con **df_boxplt**, donde `Total Est. Fee` del dataframe **df** necesita eliminar `$` y convertirse en una variable `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_est_fee=df['Total Est. Fee'].str.replace('$', '').astype(float)\n",
    "df_scatter=pd.concat([ini_cost,total_est_fee], axis=1)\n",
    "df_scatter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces escribimos la siguiente línea: `df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)` pararealizar la impresión del gráfico _scatter plot_ sobre los atributos `Initia Cost` y `Total Est.Fee`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scatter.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente los _outlayers_ se ven presentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy data\n",
    "\n",
    "Existen do premisas que hacen a un Dataset organizado y consistente:\n",
    "\n",
    "* Cada atributo estará representando a variables diferentes.\n",
    "\n",
    "* Los renglones deberán alojar registros únicos e independientes.\n",
    "\n",
    "* Las dos premisas anteriores forman una Tabla.\n",
    "\n",
    "Un claro ejemolplo de esto es cuando en lugar de tener atributos (columnas) que representea variables, se tienen valores. Esto es algo que es impresindible identificar.\n",
    "\n",
    "Por ejemplo: _en el siguiente DataFrame observamos un caso tipico de inconsitencia, ya que los atributos no representan variables, si no valores de la variable \"tratamiento\"._\n",
    "\n",
    "Otra observación es que: _para el valor \"tratamiento-a\" los registros \"b\" se hacen presentes. Lo cual también indica una inconsitencia para los datos._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tratamiento=['a','b','b','b']\n",
    "paciente=['Alejandro', 'Gustavo', 'Claudia', 'Mónica']\n",
    "error={'tratamiento-a':tratamiento, 'beneficiario':paciente}\n",
    "dict_error=pd.DataFrame(error)\n",
    "dict_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melting\n",
    "\n",
    "En ocaciones es necesario trasformar nuestro Dataset en una tabla que tenga un mejor entendimiento o una mejor vista. Uno de los procesos más comunes es realizar un _Melting_ lo que significa transformar las columnas en renglones.\n",
    "\n",
    "Para el siguiente Dataframe los atributos `Ozone`, `Solar . R`, `Wind` y `Temp` estan representados, cada uno, en una columna. Si por alguna razón necesitamos convertir estos datos en reglones aplicamos el método _melt_. Necesitamos entonces identificar los arámetro `id_vars` y `value_vars`.\n",
    "\n",
    "* `id_vars` representa las columnas que no deseamos transponer.\n",
    "\n",
    "* `value_vars` representa las columnas que deseamos convertir en renglones.\n",
    "\n",
    "El Dataset que vamos a ocupar es en esta ocación es **airquality.csv** y mantengamos en mente que tiene la siguiente estructura (`153,6`) que es igual a `153` registros u observaciones y `6` atributos o variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality=pd.read_csv('airquality.csv')\n",
    "airquality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_metl=pd.melt(airquality, id_vars=['Month','Day'])\n",
    "airquality_metl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos renombrar los atributos (columnas) una vez usado **melting** en el nuevo _Dataframe_. Solo necesitamos los siguientes parámetros:\n",
    "\n",
    "* `var_name = 'new_name'` representará el nuevo nombre para nuestro atributo.\n",
    "\n",
    "* `value_name = 'new_name'` que será el nombre para la nueva columna de los valores transpuestos de nuestro nuevo atributo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "airquality_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot\n",
    "\n",
    "Mientras _melting_ convierte atributos en renglones _pivot_ crea un nuevo atributo, con los valores únicos, a partir de un atributo seleccionado.\n",
    "\n",
    "`.pivot_table()` necesita los siguientes parámetros:\n",
    "\n",
    "* `index` será el atributo o atributos que no serán tomados en cuenta en el pivoteo.\n",
    "\n",
    "* `colums` el nombre de la columna o columnas que deseamos utilizar como pivote.\n",
    "\n",
    "* `values` los valores que serán utilizados pra el atributo que utilizamos como pivote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_pivot =airquality_melt.pivot_table(index=['Month','Day'], columns='measurement', values='reading')\n",
    "airquality_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder regresar al Dataframe original utilizamos el método `reset_index()`. Sin embargo necesitamos crear un supusesto escenario donde accidentalmente los datos en nuestro dataset **airquality** se duplicaron. Las siguientes líneas crearán renglones duplicados para este ejercicio y podremos observar al final que; para el atributo `reading=41` los datos se duplican.\n",
    "\n",
    "Los datos duplicados estarán alojados en un _Dataframe_ de nombre **airquality_dup**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_dup=airquality_melt\n",
    "airquality_dup=airquality_dup.append([airquality_dup]*3, ignore_index=True) \n",
    "airquality_dup[airquality_dup['reading']==41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí podemos observar que **airquality_pivot** está bajo la operación `pivot_table()` anteriormente realizada. Su construcción es de tipo (`153,4`). Necesesitamos regresar al punto inicial donde la tabla **airquality**, antes de ser procesada varias ocaciones, tenia una construcción (`153,6`).\n",
    "\n",
    "`reset_index()` necesita los siguientes parámetros:\n",
    "\n",
    "* `index` serán las columnas que deseamos transponer.\n",
    "\n",
    "* `values` los valores que serán utiliazados como registros en nuestros atributos después de realizar el pivoteo al Dataset.\n",
    "\n",
    "* `column` la columna pivote.\n",
    "\n",
    "* `aggfunc` este método acepta la función de agregación **mean** con la cual agrupará a los datos duplicados por. \n",
    "\n",
    "Al final observaremos que **airquality_pivote** tiene de nueva cuenta los mísmo renglones y las mismas columnas como en un principio con el _Dataframe_ **airquality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "airquality_pivot = airquality_dup.pivot_table(index=['Month','Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "airquality_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_melting_ y _pivot_ son herramientas que necesitamos continuamente cuando es necesario hacer un _reshaping_. Otro problema muy común es cuando en un mismo atributo (columna) tenemos multiple información alojada. \n",
    "\n",
    "#### Separación de caracteres\n",
    "\n",
    "La tabla **tuberculosis** aloja registros de casos reportados para los atributos `country`, `year`, `gender` y grupo de edades `age group`. El ejercicio  para esta ocasión es obtener dos atributos uno para el simple _genero, `m`, y otra para el rango de edad `0-14...`. Esto se logrará separando los caracteres del título del atributo `age group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb=pd.read_csv('tuberculosis.csv')\n",
    "tb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder separar este atributo en dos columnas necesitamos realizar un _melting_ pasando como parámetro `id_vars` a `country` y `years` ya que no queremos que estos atributos sufran un cambio. Este proceso será alojado en un nuevo _DataFrame_ llamado **tb_melt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_melt=pd.melt(tb, id_vars=['country','year'])\n",
    "tb_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después es necesario crear los nuevos atributos `gender` y `age_group`. Para ello es necesario apoyarnos de los `slicing` para poder acceder a la posición de los registros en el atributo `variable` del nuevo _Dataframe_ **tb_melt**. Esto lo podemos realizar de la siguiente manera: `DataFrame['new_colum]=DataFrame.variable.str[slicing or string position]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_melt['gender']=tb_melt.variable.str[0]\n",
    "tb_melt['age_group']=tb_melt.variable.str[1:]\n",
    "tb_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Separación con los métodos split y get.\n",
    "\n",
    "Observemos los atributos del siguiente _DataFrame_ **ebola**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola=pd.read_csv('ebola.csv')\n",
    "ebola.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos que los títulos se componen de dos partes _Cases_ o _Deaths_ y la segunda parte por _Country_. Si queremos separar los guiones bajos `_` no podemos realizarlo como en el ejercicio anterior. Es necesario utilizar métodos _built-string_ que divide en varias partes una cadena.\n",
    "\n",
    "Por ejemplo: si al atrubuto `atribute_colum` con registros `Char1_Char2` le aplicamos el meétodo `Cases_Guinea.split('_')`, sobre un nuevo atributo (`atribute_splited`) que anteriormente hayamos creado, tendremos como resultado la división de la cadena ~~`Char1_Char2`~~ según el número de `_` que contengan nuestros registros. Estos serán alojados en una lista (`['Char1','Char2']`) y serán los nuevos registros para el nuevo atributo creado anteriormente (`atribute_splited`). De esta manera apoyados de `.str` y `get()` podremos acceder a los elementos de esta lista y posteriormente separarlos como regitros independientes.\n",
    "\n",
    "Nuestro nuevo ejercicio será reconstruir, en un _Dataframe_ nuevo, el dataset **ebola**. Este _Dataframe_ deberá contener un atributo `type` para los rcaracteres `Cases` o `Death` y un atributo `Country` para el país según sea el caso. Antes que nada necesitamos procesar el Dataframe **ebola** aplicando _melting_. Anteriormente vimos que la sintaxis para ejecutar el método `melt` era la siguiente: `DataFrame['new_colum]=pd.melt(Dataframe, id_vars=['atribute_col1', 'atribute_col2'], var_name='new_atribute_name', value_name='new_atribute_name')`\n",
    "\n",
    "Los parámetros en esta ocasión serán:\n",
    "\n",
    "* `id_vars=['Date','Day']` ya que son los atributos que no veran cambios.\n",
    "\n",
    "* `var_name='type_country'` que es el nuevo atributo donde se volcarán todos los demás atributos.\n",
    "\n",
    "* `value_name='counts'` alojará el registro de los atributos volcados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_melt=pd.melt(ebola, id_vars=['Date','Day'], var_name='type_country', value_name='counts')\n",
    "ebola_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es dividir con el metodo `.split()`, el nuevo atributo `type_country`, en dos. Pero antes es necesario crear otro nuevo atributo más (`str_split`) que alojará a la lista con las cadenas divididas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "ebola_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es turno de crear los atributos finales `type` y `country`. Es aquí donde utilizarémos a los métodos `.str` y `.get()` para extraer los elementos de la lista y asignarlos a los atributos correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "ebola_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenación de datos\n",
    "\n",
    "#### Combinación de datos.\n",
    "\n",
    "El Dataset **nyc_uber_2014.csv** contiene datos de **Nueva York** de **UBER**. Este ejercicio constará en reunir _concatenar_ 3 archivos **uber_4**, **uber_5** y **uber_6** en uno solo, parecido al de **nyc_uber_2014.csv**\n",
    "\n",
    "Sin embargo antes de empezar es necesario la creación de los archivos que contengan los datos para cada mes:\n",
    "\n",
    "* El primer paso es poder manipular mejor el _Dataset_  y para eso es necesario cambiar de nombre al atributo `Data/Time` por `dt_time`.\n",
    "\n",
    "* Hay que identificar cada mes en el _Dataset_ por mese, podemos hacer esto usando `.split` y `get()` sobre el nuevo atributo `dt_time`.\n",
    "\n",
    "* Creamos 3 datasets de nombre **uber_n**, donde **n** es el número correspondiente del mes.\n",
    "\n",
    "El código de estas operaciónes se muestran acomtinuación:\n",
    "\n",
    "* Creación del atributo `dt_time`:\n",
    "\n",
    "    ```python\n",
    "    uber=pd.read_csv('nyc_uber_2014.csv')\n",
    "    uber=uber.rename(columns ={'Date/Time':'dt_time'})\n",
    "    uber['dt_split']=uber.dt_time.str.split('/')\n",
    "    ```\n",
    "    \n",
    " \n",
    "* Construcción del _Dataset_ **uber_n**, para **n** igual al mes correspondiente:\n",
    "\n",
    "    ```python\n",
    "    uber['month'] = uber.dt_time.str[0].astype(int)\n",
    "    uber_n=uber[uber['month']==n]\n",
    "    uber_n.drop(['dt_split','month'],axis=1).to_csv('uber_n.csv',sep=',', index=0)\n",
    "    ```\n",
    "\n",
    "Lo siguiente será cargar cada uno de estos _Dataframes_ para poder trabajr con ellos. Desúés se realizará un procedimiento para no tener que cargar los _Datasets_ individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber4=pd.read_csv('uber_4.csv')\n",
    "uber5=pd.read_csv('uber_5.csv')\n",
    "uber6=pd.read_csv('uber_6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de haber cargado los _Dataset_ en variables individuales es hora de utilizar `pd.concat()` que resivirá, como parámetros, una lista con cada una de las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_concat=pd.concat([uber4,uber5,uber6])\n",
    "row_concat.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinación de atributos.\n",
    "\n",
    "Para el siguiente ejemplo necesitamos crear dos _Datasets_ a partir de `ebola_melt`, `ebola_melt2` y `status_country`. \n",
    "\n",
    "La atarea es unir estos dos _Datasets_, con varios atributos, en uno solo. Necesitamos usar el método `concat` cuyos parámetros son: una lista con los _Dataframes_ y el parámetro `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_melt2=ebola_melt.iloc[:,0:3]\n",
    "status_country=ebola_melt.iloc[:,5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo _Dataframe_ se llamará **ebola_tidy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola_tidy=pd.concat([ebola_melt2,status_country], axis=1)\n",
    "ebola_tidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busqueda y concatenación de patrones.\n",
    "\n",
    "#### Coicidencia de patrones.\n",
    "\n",
    "En ocaciones es necesario concatenar una gran variedad de archivos o _Datasets_. Realizarlo manualmente sería una tarea poco funcional. \n",
    "\n",
    "Sin embargo _Python_ nos provee de funciones que nos ayudan a realizar estas tareas de una manera outomatizada.\n",
    "\n",
    "<br>\n",
    "<blockquote>\n",
    "    <p style=\"font-style: italic; color:#737373; text-align:justify\"> &mdash; El módulo glob busca las rutas de archivos que hagna coicidir con una serie de caracteres (wildcarts) parecidas al de \"shell\" en UNix como *,?,etc,etc... &mdash;\n",
    "</p>\n",
    "</blockquote><div style=\"text-align:right; width:100%\"><cite style=\"font-style: italic; color:#737373; text-align:right\">– Manual de Python...</cite></div>\n",
    "<br>    \n",
    "</div>\n",
    "</font>\n",
    "\n",
    "En el ejercicio anterior donde teníamos que cargar los diferentes _Datasets_ de **UBER** para cada uno de los meses, y con ellos poder construir un solo _Dataframe_, era necesario cargar uno por uno. Pero si quisiéramos realizar la concatenación de diferentes _Datasets_ sin tener que hacerlo uno por uno basta con realizar los siguientes pasos:\n",
    "\n",
    "* Asegurarse que la función **glob** este cargada para poder trabajar con ella.\n",
    "\n",
    "* Creamos una nueva variable llamada **pattern** que alojará, en una cadena y acompañado del wildcart `*` más el patron de archivos que agrupará. Por ejemplo `uber*.csv`.\n",
    "\n",
    "* Los nombres de los diferentes archivos serán guardados en una lista. Para ello es necesario utilizar el método `.glob` de la función `glob()`. Ejmplo `csv_files = glob.glob(pattern)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "pattern = 'uber*.csv'\n",
    "csv_files = glob.glob(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí hemos podido indicarle a _Python_ que es necesario que agrupe todos archovos de formato `.csv` por que queremos realizar una unión de todos ellos. Ahora solo resta ingresar a cada uno de los nombres para corroborar que todos se encuentren cargados y listos para usarse.\n",
    "\n",
    "```python\n",
    "print(csv_files)\n",
    "['uber_4.csv', 'uber_5.csv', 'uber_6.csv']\n",
    "```\n",
    "\n",
    "Para acceder a uno de llos es necesario ejecutar la consulta de la siguiente manera:\n",
    "\n",
    "```python\n",
    "some_csv=pd.read_csv(csv_files[index])\n",
    "some_csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_csv=pd.read_csv(csv_files[1])\n",
    "uber_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteración y concaqtenación\n",
    "\n",
    "Ahora que ya tenemos una lista de _Datasets_ para poder utilizar podemos realizar un loop, como anteriromente, habíamos planteado para poder unirlos y no tener que definirlos uno por uno para que podamos juntarlos.\n",
    "\n",
    "* En primer lugar necesitamos una lista vacia para poder iterar a cada archivo **.csv**. La llamaremos `frames = []`\n",
    "\n",
    "* Después creamos el loop para poder iterar a cada uno de los archivos. Ejemplo:\n",
    "\n",
    "```python\n",
    "for iter in frames:\n",
    "    DataFrame=pd.read_csv(iter)\n",
    "    frames.append(DataFrame)\n",
    "uber=pd.concat(frames)\n",
    "```\n",
    "\n",
    "Como podemos observar el loop realizará la extracción, la lectura y la concatenación de todos los archivos por nocotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[]\n",
    "for csv in csv_files:\n",
    "    df=pd.read_csv(csv)\n",
    "    frames.append(df)\n",
    "uber=pd.concat(frames)\n",
    "uber.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruce de datos (Merge).\n",
    "\n",
    "\n",
    "La concatenación no es el único método que se puede utilizar a la hora de combinar datos. Sobre todo cuando el atributo que deseamos concatenar no tiene el mísmo orden en ambos atributos del _Datasets_. \n",
    "\n",
    "`.merge()` es una función que le permite a _Pandas_ poder realizar el cruce de datos, con base en un atributo presente en ambos _Datasets_, de manera similar que con _SQL_. La consulta se llevaría a cabo con los siguientes parámetros:\n",
    "\n",
    "* `left=dataset` y `rigth=dataset` ambos deberán contener el mismo atributo y pueden tener, o no, el mismo nombre.\n",
    "\n",
    "* `on=None` se utiliza cuando el atributo objetivo aparece con el mismo nombre en ambos _Datasets_. Si no el valor será `None`.\n",
    "\n",
    "* `left_on='atribute_a'` & `rigth_on='atribute_b'` cuando el atributo objetivo aparece, en ambos datasets, con diferente nombre.\n",
    "\n",
    "Existen diferentes maneras de realizar un _join_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge 1-to-1.\n",
    "\n",
    "Imaginemos que necesitamos realizar el cruze de la información de dos _Datasets_ `visited` & `site`. Si observamos, en ambas columnas, existe un atributo que comparten ambas tablas pero con diferente nombre. Uno es `name` en el _Dataset_ `site` y otro es `site` en el _Dataset_ `site`. El resultado es necesario guardarlo en un nuevo _Dataset_ llamada **o2o**.\n",
    "\n",
    "Para poder cruzar los datos en el atributo que tienen en particualr ambas tablas la línea sería: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = pd.read_csv('site.csv')\n",
    "visited = pd.read_csv('visited.csv')\n",
    "o2o=pd.merge(left=site, right=visited, on=None, left_on='name', right_on='site')\n",
    "o2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge many to one (one to many).\n",
    "\n",
    "Cuando se tienen registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_visited=pd.read_csv('dup_visited.csv')\n",
    "m2o = pd.merge(left=site,right=dup_visited, left_on='name',right_on='site')\n",
    "m2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge many to many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey=pd.read_csv('dup_visited.csv')\n",
    "survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey=pd.read_csv('survey.csv')\n",
    "m2m = pd.merge(left=m2o, right=survey, left_on='ident', right_on='taken')\n",
    "m2m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips de datos.\n",
    "\n",
    "#### Conversión entre tipos de datos.\n",
    "\n",
    "En muchas ocaciones es necesario realizar la conversión entre diferentes tipos de datos. El tipo de dato `object` es el tipo de dato que _Python_ utiliza para codificar caracteres. Los atributos numéricos pueden ser transformados a caracteres y viceversa. El método `astype()` nos ayuda a realizar la conversión.\n",
    "\n",
    "Por lo general cuando leemos _Datasets_ damos por hecho que cada uno de los atributos que estámos cargando son los correctos. En otras ocasiones necesaitamos cambiar el tipo de dato, en uno o varios atributos, para poder trabajar con sus datos. El parámetro, de la fucnión `read_csv`, que me permite realizar esto es `dtype = {'atribute1':select_dtype,'atrubute2':select_dtype,...}`.\n",
    "\n",
    "Para efectos de este ejercicio leeremos el archivo **tups.csv** para el cual es necesario cambiar el tipo de dato a los atributos `total_bill` y `tip`. Después chequemos la información con la fucnión `info()` para observar los diferentes tipos de datos de los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips=pd.read_csv('tips.csv', dtype={'total_bill':object,'tip':object} )\n",
    "tips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra de las ventajas de hacer cambios en los tipos de datos es el ganar memoria. En algunos casos el tipo de dato que presenta un atributo no es el más funcional para nuestro análisis. Por ejemplo; el mejor tipo de dato para el atributo  `sex` o `smoker` sería `'category'` en lugar de `object`.\n",
    "\n",
    "Podemos realizar el cambio utilizando la función `astype()` y el argumento `'category'`. Si de nueva cuenta checamos la información del _Dataset_ para observar los cambios, además de reducir el tamaño de uso en emoria de `13.4+ KB` a `10.3+ KB`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.sex = tips.sex.astype('category')\n",
    "tips.smoker=tips.smoker.astype('category')\n",
    "tips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de caracteres.\n",
    "\n",
    "#### Empate de caracteres con expresiones regulares.\n",
    "\n",
    "La mayoría de los _Datasets_ están construidos por caracteres. Es por eso que su manipulación se vuelve de suma importancia para pocer hacer consistente nuestros datos y poder trabajr con ellos.\n",
    "\n",
    "_Python_ proporciona herramientas para poder manipular caracteres, como el modulo `re` (expresiones regulares o regex) cuyo trabajo es empatar secuencias de caracteres. Es parecida al modulo `glob` y las _wildcarts_. \n",
    "\n",
    "A continuación ejemplificaremos alguas secuencias de caracteres y sus correspondientes expresiones regulares::\n",
    "\n",
    "* `17` cantidades con datos de tipo entero pueden ser reemplazados por la expresión regular: `\\d*`.\n",
    "\n",
    "* `$17` para empatar cantidades como la anterior, pero con el signo `$`, son equivalentes a colocar: `\\$\\d*`.\n",
    "\n",
    "* `$17.00` números enteros, con \"n\" cantidad de decimales, son equivalentes a tener: `\\$\\d*\\.\\d*`.\n",
    "\n",
    "* `$1234567.12 ` esta catidad contiene muchos dígitos, y después del punto decimal solo dos. Podemos localizar estas cantidades: `\\$\\d*\\.\\d{2}`\n",
    "\n",
    "* `$1234567.123` con cantidades de más de 3 dígitos la consulta debería de ser así: `^\\$\\d*\\.\\{2}$`\n",
    "\n",
    "* Utilizamos `[A-Z]` para hallar mayúsculas, seguida de `\\w*` que busca \"n\" cantidad de datos alfanuméricos en una cadena. \n",
    "\n",
    "Averiguemos si, es posible saber si dado el formato de un código de la forma `xxx-xxx-xxx` coincide si ingresamos un código manualmente. Para ello utilizamos el modulo `re` y su método `compile` quien alojará el formato del código de caracteres. El metodo `match` realiza la comparación y nos regresa un resultado de tipo _Booleano_.\n",
    "\n",
    "Probemos con dos códigos: `1123-456-7890` y `123-456-7890`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "prog = re.compile('xxx-xxx-xxxx')\n",
    "result = prog.match('123-456-7890')\n",
    "result2 = prog.match('123-456-7890')\n",
    "print(bool(result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción de valores numéricos desde caracteres.\n",
    "\n",
    "Esta tarea es, a menudo, muy realizada. En la siguiente cadena: `la receta lleva 10 fesas y 1 platano` nos interesa extraer los valores `10` y `1`. Cuando necesitamos extraer valores con varios, donde los caracterers corresponden a varios patrones de coincidencias, la fucnión `findall()` nos es de gran ayuda. Esta nos regresa, en una lista, todas las coincidencias que hayamos indicado encontrar.\n",
    "\n",
    "Entonces `\\d` nos ayuda a encontrar números. Esta _regex_ es precedida de `+` para que se repita varias veces la búsqueda. Esto nos asegura que el número `10` es visualizado como un número solo y no la combinación de `1` y `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall('\\d+', 'la receta lleva 10 fesas y 1 platano')\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconocimiento para el formato de un número telefónico de la forma `xxx-xxx-xxx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "pattern1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconocimiento, en una cadena de caracteres, números con el formato de dos cifras decimales `$123.45`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = bool(re.match(pattern='\\$\\d*\\.\\d{2}', string='$123.45'))\n",
    "pattern2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coincidencia de letras mayúsculas en la palabra `Australia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern3 = bool(re.match(pattern='[A-Z]\\w*', string='Australia'))\n",
    "pattern3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rutinas para la limpieza de datos\n",
    "\n",
    "Podemos automatizar la limpieza en un _Dataset_ programando rutinas que hacen eficiente este proceso. Por ejemplo; el _Dataset_ **nan.csv** contiene 5 columnas donde se alojan records de tipo `NaN`.\n",
    "\n",
    "Nuestro trabajo será propgramar una rutina que: pueda reconocer si un redord, en una de las columnas del _Dataset_ **nan.csv**, es menor que `3` lo pueda etiquetar como `Malo`. Si es mayor o igual a `3` lo pueda etiquetar como `Bueno`. Pero si no cumple con ninguna de las condiciones anteriores lo pueda etiquetar como `No aplicó`.\n",
    "\n",
    "Para este ejercico necesitamos crear el _Dataset_ con valores random y para poder trabajar con el siguiente código;\n",
    "\n",
    "```python\n",
    "np.random.seed(123)\n",
    "data = np.random.randint(0, 10, (10,5))\n",
    "df_nan = pd.DataFrame(data, columns=list('abcde'))\n",
    "df_nan = df.where(df > 2)\n",
    "df_nan.to_csv('nan.csv', sep=',', index=False)\n",
    "```\n",
    "La sintaxis de la función que llamaremos como `look` necesita la siguiente estructura;\n",
    "\n",
    "```python\n",
    "# Definimos la función 'function_name' con el parámetro 'par':\n",
    "def function_name(par):\n",
    "\n",
    "    # Regresa 'respuesta' cuando condición:\n",
    "    if condición :\n",
    "        return 'Respuesta'\n",
    "    \n",
    "    # Regresa 'Bueno' cuando condición:\n",
    "    elif condición:\n",
    "        return 'Respuesta'\n",
    "    \n",
    "    # Cuando no se cumpla ninguna de las condiciones:    \n",
    "    else:\n",
    "        return 'Respuesta'\n",
    "\n",
    "# Alicamos la función creando una columna al dataframe:\n",
    "df['new_colum'] = df.column.apply(function_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_look=pd.read_csv('nan.csv')\n",
    "def look(val):\n",
    "\n",
    "    if val <= 3:\n",
    "        return 'Malo'\n",
    "    \n",
    "    elif val >3:\n",
    "        return 'Bueno'\n",
    "    \n",
    "    else:\n",
    "        return '$No aplicó'\n",
    "    \n",
    "df_look['columna_a'] = df_look.a.apply(look)\n",
    "df_look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que en el nuevo _Dataframe_, la columna `columna_a`, contiene records `$No aplicó`. Ahora supongamos que necesitamos limpiar esos records, pero en lugar de hacerlo como en el paso anterior nos apoyaremos con la función `replace()` y con la ayuda de la construcción de una función **lambda**. \n",
    "\n",
    "Las funciones **lambda** tienen la siguiente sintaxis:\n",
    "\n",
    "```python\n",
    "df['new_column'] = df.column.apply(lambda iter: iter.replace('$', ''))\n",
    "```\n",
    "donde `iter` es nuestro valor para iterar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_look['nueva_columna_a'] = df_look.columna_a.apply(lambda x: x.replace('$', ''))\n",
    "df_look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajar con valores duplicados y valores de tipo \"NaN\".\n",
    "\n",
    "#### Eliminación de valores duplicados.\n",
    "\n",
    "Trabajar con valores duplicados puede traernos varios inconvenientes. Ocupan especio extra en la memoria y pueden producir errores en nuestros cálculos. \n",
    "_**Pandas**_ nos provee de funciones que nos ayudan a la eliminación de estos valores. Para este ejercicio crearemos un nuevo _Dataframe_ llamado **df_lookrep** con algunos valores repetidos. EL objetivo será eliminar estos registros aplicandole el método `drop_duplicates()`. \n",
    "\n",
    "Según la información del tataframe **df_look** debemos tener un total de 10 renglones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookrep = pd.concat([df_look]*3, ignore_index=True)\n",
    "df_lookrep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se podrá ver el _Dataframe_ **clean_dflook** volverá a tener los mismos renglones que en un principio tenia **df_look**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dflook=df_lookrep.drop_duplicates()\n",
    "clean_dflook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elinación de valores de tipo \"NaN\".\n",
    "\n",
    "Es muy raro trabajar con datasets que no contengan algún tipo de _missing values_. Se debe tener en cuenta la naturalesa de nuestros datos o su origen para posteriormente tomar las correctas desiciones para poder trabajr con ellos. SObre todo poer que en muchos casos los cálculos no aceptan este tipo de records.\n",
    "\n",
    "El método `fillna()` nos ayuda a trabajar con los atributos y sustituir estos registros por cualquier otro valor que nosotros necesitemos. Por ejemplo; el _Dataset_ **airquality** tiene un total de **153** registros, pero en el atributo `Ozone` tenemos **116**. Lo cual indica que de alguna manera existen records de tipo `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a sustiruir estos records con el valor del promedio del atributo `Ozone`. Para ello necesitaremos ayuda del método `.mean()`. Nuestro nuevos cálculos serán alojados en el mismo _Dataframe_ y en el msimo atributo llamado `Ozone`. Al terminar veremos en la información, del nuevo _Dataset_, que el nuevo valor para el atributo `Ozone` será de **153**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oz_mean=airquality.Ozone.mean()\n",
    "airquality['Ozone']=airquality.Ozone.fillna(oz_mean)\n",
    "airquality.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación en _Datasets_ con \"assest\".\n",
    "\n",
    "Esta rutina nos ayudará a verificar si existen records de tipo (`NaN`) en todos los atributos de un _Dataset_. Para ello nos apoyaremos de dos métodos más: `notnull()` y `all()`. `assert` es la función que evaluará esta rutina.\n",
    "\n",
    "**Nota: podemos utilizar el equivalente que es pd.notnull(df)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos entonces: ¿qué atributos contienen registros `NaN` o `missing values` en el _Dataset_ **ebola**?.. Estos serán etiquetados con `False` en caso de ser así. De momento no abordaremos la explicación del método `all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notnull(ebola).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gran parte de este _Dataframe_ contiene records de tipo `NaN`. Para este ejercicio utilizaremos el método `fillna()` para sustituirlos por `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ebolafill=ebola.fillna(0)\n",
    "df_ebolafill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si ejecutamos de nueva cuenta `pd.notnull(Dataframe).all()` observaremos que todos los atributos tienen records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notnull(df_ebolafill).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `all()` nos devuelve `True` cuando todos los registros en una consulta son `True`. Cuando se usa `all()`, en un _Dataframe_, para cada uno de los atributos se devuelve una seríe de registros _Booleanos_ (`False` o `True`). Por eso es necesario encadenarlo a otro método `all()`, de nueva cuenta, para que solo regrese `True` si todos los registros en los atributos fueron _True_. \n",
    "\n",
    "De esta manera es como podemos verificar que todos los records de un _Dataset_ no contienen valores `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.notnull(df_ebolafill).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.6\n",
      "IPython 7.12.0\n",
      "\n",
      "pandas 1.0.1\n",
      "watermark 2.0.2\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-97-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      " \n",
      "last updated: Sat Apr 18 2020 22:47:54 CDT\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "%watermark -v -m -p pandas,watermark\n",
    "\n",
    "print(\" \")\n",
    "%watermark -u -n -t -z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
